{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9413e239",
      "metadata": {
        "id": "9413e239"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import AirfRANS\n",
        "from torch_geometric.loader import DataLoader # We'll use this later for batching\n",
        "\n",
        "# Define a root directory for the dataset\n",
        "# This is where the dataset files will be downloaded and stored.\n",
        "DATA_ROOT = './data/AirfRANS'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb23f5b1",
      "metadata": {
        "id": "eb23f5b1",
        "outputId": "8039aed5-1355-49cc-8c4b-d9a150056f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading AirfRANS training dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.isir.upmc.fr/extrality/pytorch_geometric/AirfRANS.zip\n",
            "Extracting data\\AirfRANS\\raw\\AirfRANS.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 800 training samples.\n",
            "Loading AirfRANS test dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 200 test samples.\n",
            "\n",
            "Inspecting the first training data sample:\n",
            "Data(x=[181794, 5], y=[181794, 4], pos=[181794, 2], surf=[181794], name='airFoil2D_SST_36.622_11.319_3.941_5.424_1.0_16.283')\n"
          ]
        }
      ],
      "source": [
        "# Load the AirfRANS dataset\n",
        "# 'task=\"full\"' means we're using the full dataset for interpolation tasks (learning general behavior).\n",
        "# 'train=True' loads the training split, 'train=False' loads the test split.\n",
        "# We'll apply graph construction and normalization transforms in the next steps.\n",
        "print(\"Loading AirfRANS training dataset...\")\n",
        "train_dataset = AirfRANS(root=DATA_ROOT, task='full', train=True, transform=None) # transform=None for now\n",
        "print(f\"Loaded {len(train_dataset)} training samples.\")\n",
        "\n",
        "print(\"Loading AirfRANS test dataset...\")\n",
        "test_dataset = AirfRANS(root=DATA_ROOT, task='full', train=False, transform=None) # transform=None for now\n",
        "print(f\"Loaded {len(test_dataset)} test samples.\")\n",
        "\n",
        "# Let's inspect the first data sample from the training set\n",
        "print(\"\\nInspecting the first training data sample:\")\n",
        "first_data_sample = train_dataset[0]\n",
        "print(first_data_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5957fe52",
      "metadata": {
        "id": "5957fe52",
        "outputId": "58d7ef63-71e0-4cc8-ad8a-0c90257d1522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of node features (data.x): torch.Size([181794, 5])\n",
            "Shape of node positions (data.pos): torch.Size([181794, 2])\n",
            "Shape of node targets (data.y): torch.Size([181794, 4])\n",
            "Number of nodes in the first sample: 181794\n",
            "Number of edges in the first sample (initially 0): 0\n",
            "\n",
            "Number of original node features (in_channels): 5\n",
            "Number of original output targets (out_channels): 4\n"
          ]
        }
      ],
      "source": [
        "# Understanding the Data Object:\n",
        "# PyTorch Geometric's Data object is a simple container for graph data.\n",
        "# It typically contains:\n",
        "# - data.x: Node feature matrix with shape [num_nodes, num_node_features].\n",
        "#           For AirfRANS, this includes inlet velocity (vx, vy), distance to airfoil, and airfoil normals (nx, ny). (5 features)\n",
        "# - data.pos: Node position matrix with shape [num_nodes, num_dimensions].\n",
        "#             For AirfRANS, this is [num_nodes, 2] for (x, y) coordinates.\n",
        "# - data.y: Node target matrix with shape [num_nodes, num_node_labels].\n",
        "#           For AirfRANS, this includes velocity (vx, vy), pressure/specific_mass, and turbulent_kinematic_viscosity. (4 targets)\n",
        "# - data.edge_index: Graph connectivity in COO format with shape [2, num_edges].\n",
        "#                    **IMPORTANT:** AirfRANS initially does NOT have `edge_index` by default (it's a point cloud). We'll add this in the next step!\n",
        "# - data.num_nodes: The number of nodes in the graph.\n",
        "# - data.num_edges: The number of edges in the graph. (Will be 0 for now, until we add edges)\n",
        "\n",
        "print(f\"Shape of node features (data.x): {first_data_sample.x.shape}\")\n",
        "print(f\"Shape of node positions (data.pos): {first_data_sample.pos.shape}\")\n",
        "print(f\"Shape of node targets (data.y): {first_data_sample.y.shape}\")\n",
        "print(f\"Number of nodes in the first sample: {first_data_sample.num_nodes}\")\n",
        "print(f\"Number of edges in the first sample (initially 0): {getattr(first_data_sample, 'num_edges', 0)}\") # Safely access num_edges\n",
        "\n",
        "# Check the number of original node features and output targets\n",
        "print(f\"\\nNumber of original node features (in_channels): {train_dataset.num_node_features}\")\n",
        "print(f\"Number of original output targets (out_channels): {train_dataset[0].y.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77c8d44",
      "metadata": {
        "id": "d77c8d44",
        "outputId": "29f54264-a71c-48b6-99f9-24b39cd7bf76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading AirfRANS dataset with knn graph transform...\n",
            "Using KNNGraph with k=10\n",
            "Fitting scalers incrementally using partial_fit()...\n",
            "  Processed 100/800 samples for scaler fitting.\n",
            "  Processed 200/800 samples for scaler fitting.\n",
            "  Processed 300/800 samples for scaler fitting.\n",
            "  Processed 400/800 samples for scaler fitting.\n",
            "  Processed 500/800 samples for scaler fitting.\n",
            "  Processed 600/800 samples for scaler fitting.\n",
            "  Processed 700/800 samples for scaler fitting.\n",
            "  Processed 800/800 samples for scaler fitting.\n",
            "Applying graph construction and normalization to datasets...\n",
            "Loaded 800 training samples after preprocessing.\n",
            "Loaded 200 test samples after preprocessing.\n",
            "\n",
            "Inspecting the first training data sample AFTER preprocessing:\n",
            "Data(x=[181794, 5], y=[181794, 4], pos=[181794, 2], surf=[181794], name='airFoil2D_SST_36.622_11.319_3.941_5.424_1.0_16.283', edge_index=[2, 1817940])\n",
            "Number of nodes in the first sample: 181794\n",
            "Number of edges in the first sample: 1817940\n",
            "Shape of normalized node features (data.x): torch.Size([181794, 5])\n",
            "Shape of normalized node targets (data.y): torch.Size([181794, 4])\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.transforms import Compose, KNNGraph, RadiusGraph\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.datasets import AirfRANS\n",
        "from torch_geometric.loader import DataLoader # Make sure DataLoader is imported for later use\n",
        "\n",
        "# Define a root directory for the dataset (ensure this is defined in your notebook)\n",
        "# DATA_ROOT = './data/AirfRANS'\n",
        "\n",
        "# --- Custom Normalization Class ---\n",
        "class CustomNormalize(object):\n",
        "    def __init__(self, x_scaler, y_scaler):\n",
        "        self.x_scaler = x_scaler\n",
        "        self.y_scaler = y_scaler\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # Ensure data is on CPU before converting to numpy for StandardScaler\n",
        "        # This is important if your original data.x and data.y are already on GPU\n",
        "        data.x = torch.tensor(self.x_scaler.transform(data.x.cpu().numpy()), dtype=torch.float)\n",
        "        data.y = torch.tensor(self.y_scaler.transform(data.y.cpu().numpy()), dtype=torch.float)\n",
        "        return data\n",
        "\n",
        "# --- Data Loading and Preprocessing Function ---\n",
        "def load_and_preprocess_airfrans(graph_transform_type='knn', k=10, r=0.1):\n",
        "    print(f\"Loading AirfRANS dataset with {graph_transform_type} graph transform...\")\n",
        "\n",
        "    if graph_transform_type == 'knn':\n",
        "        graph_transform = KNNGraph(k=k)\n",
        "        print(f\"Using KNNGraph with k={k}\")\n",
        "    elif graph_transform_type == 'radius':\n",
        "        graph_transform = RadiusGraph(r=r)\n",
        "        print(f\"Using RadiusGraph with r={r}\")\n",
        "    else:\n",
        "        raise ValueError(\"graph_transform_type must be 'knn' or 'radius'\")\n",
        "\n",
        "    # Step 1: Load initial datasets WITHOUT transforms to fit scalers on raw data\n",
        "    # We only fit scalers on the TRAINING data to avoid data leakage.\n",
        "    initial_train_dataset = AirfRANS(root=DATA_ROOT, task='full', train=True, transform=None)\n",
        "\n",
        "    # Initialize scalers\n",
        "    x_scaler = StandardScaler()\n",
        "    y_scaler = StandardScaler()\n",
        "\n",
        "    print(\"Fitting scalers incrementally using partial_fit()...\")\n",
        "    # Fit scalers incrementally on training data to avoid MemoryError\n",
        "    for i, data in enumerate(initial_train_dataset):\n",
        "        # partial_fit expects a 2D array, so we pass data.x.numpy() and data.y.numpy() directly\n",
        "        x_scaler.partial_fit(data.x.numpy())\n",
        "        y_scaler.partial_fit(data.y.numpy())\n",
        "        if (i + 1) % 100 == 0 or (i + 1) == len(initial_train_dataset):\n",
        "            print(f\"  Processed {i + 1}/{len(initial_train_dataset)} samples for scaler fitting.\")\n",
        "\n",
        "    # Create the custom normalization transform using the fitted scalers\n",
        "    normalize_transform = CustomNormalize(x_scaler, y_scaler)\n",
        "\n",
        "    # Step 2: Compose final transforms (graph construction + normalization)\n",
        "    train_transform_final = Compose([graph_transform, normalize_transform])\n",
        "    test_transform_final = Compose([graph_transform, normalize_transform]) # Use train_scalers for test data!\n",
        "\n",
        "    # Step 3: Re-load datasets with both graph construction and normalization transforms applied\n",
        "    print(\"Applying graph construction and normalization to datasets...\")\n",
        "    train_dataset_preprocessed = AirfRANS(root=DATA_ROOT, task='full', train=True, transform=train_transform_final)\n",
        "    test_dataset_preprocessed = AirfRANS(root=DATA_ROOT, task='full', train=False, transform=test_transform_final)\n",
        "\n",
        "    print(f\"Loaded {len(train_dataset_preprocessed)} training samples after preprocessing.\")\n",
        "    print(f\"Loaded {len(test_dataset_preprocessed)} test samples after preprocessing.\")\n",
        "\n",
        "    return train_dataset_preprocessed, test_dataset_preprocessed, x_scaler, y_scaler\n",
        "\n",
        "# --- Execute the preprocessing for Day 1 ---\n",
        "# Define DATA_ROOT here again in case you are running this in a fresh cell\n",
        "DATA_ROOT = './data/AirfRANS'\n",
        "\n",
        "# We'll use KNNGraph for the baseline (k=10 as a good starting point)\n",
        "train_dataset_preprocessed, test_dataset_preprocessed, x_scaler_day1, y_scaler_day1 = load_and_preprocess_airfrans(graph_transform_type='knn', k=10)\n",
        "#  Original:\n",
        "# train_dataset_preprocessed, test_dataset_preprocessed, x_scaler_day1, y_scaler_day1 = load_and_preprocess_airfrans(graph_transform_type='knn', k=10)\n",
        "\n",
        "# MODIFIED for faster testing (e.g., use 5 training samples and 2 test samples)\n",
        "# Slicing the dataset will create a Subset object.\n",
        "# train_dataset_subset = train_dataset_preprocessed[:5]\n",
        "# test_dataset_subset = test_dataset_preprocessed[:2]\n",
        "# Inspect a sample again to see the added edges and normalized data\n",
        "print(\"\\nInspecting the first training data sample AFTER preprocessing:\")\n",
        "first_data_sample_preprocessed = train_dataset_preprocessed[0]\n",
        "print(first_data_sample_preprocessed)\n",
        "\n",
        "print(f\"Number of nodes in the first sample: {first_data_sample_preprocessed.num_nodes}\")\n",
        "print(f\"Number of edges in the first sample: {first_data_sample_preprocessed.num_edges}\")\n",
        "print(f\"Shape of normalized node features (data.x): {first_data_sample_preprocessed.x.shape}\")\n",
        "print(f\"Shape of normalized node targets (data.y): {first_data_sample_preprocessed.y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06f79e5c",
      "metadata": {
        "id": "06f79e5c"
      },
      "outputs": [],
      "source": [
        "# Slicing the dataset will create a Subset object.\n",
        "train_dataset_subset = train_dataset_preprocessed[:5]\n",
        "test_dataset_subset = test_dataset_preprocessed[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fabb27",
      "metadata": {
        "id": "17fabb27",
        "outputId": "7420fa05-3918-4501-ca8b-8855680d17fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNN Model Architecture:\n",
            "GCNBaseline(\n",
            "  (conv1): GCNConv(5, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Model moved to device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "# --- Baseline GNN Model (GCN) ---\n",
        "# This class defines a simple GCN architecture for node-level regression.\n",
        "class GCNBaseline(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCNBaseline, self).__init__()\n",
        "        # GCNConv is the Graph Convolutional Layer\n",
        "        # It takes input features, applies a linear transformation, and aggregates features from neighbors.\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels) # Deeper network with 3 layers\n",
        "\n",
        "        # Final linear layer to project the hidden features to the desired output dimension\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # data.x are node features, data.edge_index defines connections\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # First GCN layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x) # Apply ReLU activation function\n",
        "        x = F.dropout(x, p=0.5, training=self.training) # Apply dropout for regularization\n",
        "\n",
        "        # Second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        # Third GCN layer\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        # Final linear projection to get the output targets\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "# We get in_channels and out_channels from our preprocessed dataset\n",
        "# You might need to make sure 'train_dataset_preprocessed' is accessible in this cell\n",
        "# (e.g., run the loading cell above this one, or pass it as an argument if in a function)\n",
        "in_channels = train_dataset_preprocessed.num_node_features\n",
        "out_channels = train_dataset_preprocessed[0].y.shape[1] # Using the corrected way to get output targets\n",
        "hidden_channels = 64 # A common choice for hidden layer size\n",
        "\n",
        "model = GCNBaseline(in_channels, hidden_channels, out_channels)\n",
        "\n",
        "# Print the model architecture to confirm its structure\n",
        "print(f\"GNN Model Architecture:\\n{model}\")\n",
        "\n",
        "# Set the device to GPU if available, otherwise CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device) # Move the model to the chosen device\n",
        "print(f\"Model moved to device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd86dd92",
      "metadata": {
        "id": "cd86dd92"
      },
      "source": [
        "References:\n",
        "\n",
        "torch_geometric.nn.GCNConv documentation: PyTorch Geometric Docs\n",
        "https://www.google.com/search?q=https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html%23torch_geometric.nn.conv.GCNConv\n",
        "\n",
        "Graph Convolutional Networks (original paper by Kipf and Welling): Semi-Supervised Classification with Graph Convolutional Networks\n",
        "https://arxiv.org/abs/1609.02907"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f26dabf",
      "metadata": {
        "id": "8f26dabf",
        "outputId": "7b911d1b-3022-45e6-c7bc-d29cc3d3e237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DataLoaders created with batch_size=1.\n",
            "\n",
            "Optimizer and Loss Function configured.\n",
            "\n",
            "--- Starting Training Loop ---\n",
            "Training for 2 epochs on cpu.\n",
            "Epoch: 001, Train Loss: 0.4781, Test Loss: 0.5047\n",
            "Epoch: 002, Train Loss: 0.4759, Test Loss: 0.5060\n",
            "Environment setup, data loading, initial preprocessing, baseline GNN model, and basic training loop implemented.\n",
            "You should see the test loss decreasing over epochs, indicating the model is learning!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader # Ensure DataLoader is imported\n",
        "import torch.optim as optim # For the optimizer\n",
        "\n",
        "# --- Create DataLoaders ---\n",
        "# DataLoaders are essential for batching your graphs, which is crucial for\n",
        "# efficient training and often for memory management, even with batch_size=1\n",
        "# for very large graphs.\n",
        "batch_size = 1 # Start with a small batch size, especially with large individual graphs\n",
        "# train_loader = DataLoader(train_dataset_preprocessed, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset_preprocessed, batch_size=batch_size, shuffle=False)\n",
        "# Use these subsets in your DataLoader\n",
        "train_loader = DataLoader(train_dataset_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nDataLoaders created with batch_size={batch_size}.\")\n",
        "\n",
        "# --- Training and Testing Functions ---\n",
        "# We'll define functions to encapsulate the training and testing logic for one epoch.\n",
        "\n",
        "def train():\n",
        "    model.train() # Set the model to training mode (enables dropout, etc.)\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device) # Move graph data to the same device as the model\n",
        "        optimizer.zero_grad() # Clear gradients from the previous step\n",
        "        out = model(data)     # Perform a forward pass\n",
        "        loss = criterion(out, data.y) # Compute the loss (MSE between prediction and ground truth)\n",
        "        loss.backward()       # Compute gradients (backpropagation)\n",
        "        optimizer.step()      # Update model parameters\n",
        "        total_loss += loss.item() * data.num_graphs # Accumulate loss, weighted by number of nodes in the graph\n",
        "    return total_loss / len(train_loader.dataset) # Return average loss per node over the dataset\n",
        "\n",
        "def test(loader):\n",
        "    model.eval() # Set the model to evaluation mode (disables dropout, etc.)\n",
        "    total_loss = 0\n",
        "    with torch.no_grad(): # Disable gradient computation for efficiency and consistency\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset) # Return average loss per node over the dataset\n",
        "\n",
        "# --- Configure Optimizer and Loss Function ---\n",
        "# Ensure 'model' and 'device' are accessible from previous cells (run them first!)\n",
        "# model = model.to(device) # This line was already executed in Step 4\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer is a good general choice\n",
        "criterion = torch.nn.MSELoss() # Mean Squared Error is standard for regression tasks\n",
        "\n",
        "print(\"\\nOptimizer and Loss Function configured.\")\n",
        "\n",
        "# --- Training Loop and Initial Evaluation ---\n",
        "epochs = 2 # Start with a small number of epochs for initial testing\n",
        "\n",
        "print(\"\\n--- Starting Training Loop ---\")\n",
        "print(f\"Training for {epochs} epochs on {device}.\")\n",
        "\n",
        "# Store losses for plotting later (optional but good practice)\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train()\n",
        "    test_loss = test(test_loader) # Evaluate on the test set\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "\n",
        "print(\"Environment setup, data loading, initial preprocessing, baseline GNN model, and basic training loop implemented.\")\n",
        "print(\"You should see the test loss decreasing over epochs, indicating the model is learning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed4ac5d",
      "metadata": {
        "id": "1ed4ac5d"
      },
      "source": [
        "References\n",
        "\n",
        "torch_geometric.loader.DataLoader documentation: PyTorch Geometric Docs\n",
        "https://www.google.com/search?q=https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html%23torch_geometric.loader.DataLoader\n",
        "\n",
        "PyTorch Optimizers (e.g., torch.optim.Adam): PyTorch Docs\n",
        "https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "PyTorch Loss Functions (e.g., torch.nn.MSELoss): PyTorch Docs\n",
        "https://www.google.com/search?q=https://pytorch.org/docs/stable/nn.html%23loss-functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc11679",
      "metadata": {
        "id": "bbc11679",
        "outputId": "17916c0a-cc41-4017-ef47-273e02555bf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\phani\\OneDrive\\Aerodynamic-_Prediction_with_GNN\\data\\AirfRANS\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.abspath('./data/AirfRANS'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30448c65",
      "metadata": {
        "id": "30448c65",
        "outputId": "13e73008-4def-47ad-d996-c4a4977e0d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n",
            "--- Experiment 1.1: KNNGraph (k=10) ---\n",
            "Loading AirfRANS dataset with knn graph transform...\n",
            "Using KNNGraph with k=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.isir.upmc.fr/extrality/pytorch_geometric/AirfRANS.zip\n",
            "Extracting data\\AirfRANS\\raw\\AirfRANS.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting scalers incrementally using partial_fit()...\n",
            "  Processed 100/800 samples for scaler fitting.\n",
            "  Processed 200/800 samples for scaler fitting.\n",
            "  Processed 300/800 samples for scaler fitting.\n",
            "  Processed 400/800 samples for scaler fitting.\n",
            "  Processed 500/800 samples for scaler fitting.\n",
            "  Processed 600/800 samples for scaler fitting.\n",
            "  Processed 700/800 samples for scaler fitting.\n",
            "  Processed 800/800 samples for scaler fitting.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 800 training samples after preprocessing.\n",
            "Loaded 200 test samples after preprocessing.\n",
            "--- KNNGraph (k=10) Training Set Graph Properties ---\n",
            "Total graphs: 800\n",
            "Average nodes per graph: 179908.76\n",
            "Average edges per graph: 1799087.62\n",
            "Average degree: 10.00\n",
            "\n",
            "--- Experiment 1.2: KNNGraph (k=20) ---\n",
            "Loading AirfRANS dataset with knn graph transform...\n",
            "Using KNNGraph with k=20\n",
            "Fitting scalers incrementally using partial_fit()...\n",
            "  Processed 100/800 samples for scaler fitting.\n",
            "  Processed 200/800 samples for scaler fitting.\n",
            "  Processed 300/800 samples for scaler fitting.\n",
            "  Processed 400/800 samples for scaler fitting.\n",
            "  Processed 500/800 samples for scaler fitting.\n",
            "  Processed 600/800 samples for scaler fitting.\n",
            "  Processed 700/800 samples for scaler fitting.\n",
            "  Processed 800/800 samples for scaler fitting.\n",
            "Loaded 800 training samples after preprocessing.\n",
            "Loaded 200 test samples after preprocessing.\n",
            "--- KNNGraph (k=20) Training Set Graph Properties ---\n",
            "Total graphs: 800\n",
            "Average nodes per graph: 179908.76\n",
            "Average edges per graph: 3598175.25\n",
            "Average degree: 20.00\n",
            "\n",
            "--- Experiment 1.3: RadiusGraph (r=0.01) ---\n",
            "Loading AirfRANS dataset with radius graph transform...\n",
            "Using RadiusGraph with r=0.01\n",
            "Fitting scalers incrementally using partial_fit()...\n",
            "  Processed 100/800 samples for scaler fitting.\n",
            "  Processed 200/800 samples for scaler fitting.\n",
            "  Processed 300/800 samples for scaler fitting.\n",
            "  Processed 400/800 samples for scaler fitting.\n",
            "  Processed 500/800 samples for scaler fitting.\n",
            "  Processed 600/800 samples for scaler fitting.\n",
            "  Processed 700/800 samples for scaler fitting.\n",
            "  Processed 800/800 samples for scaler fitting.\n",
            "Loaded 800 training samples after preprocessing.\n",
            "Loaded 200 test samples after preprocessing.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 155\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Experiment 1.3: RadiusGraph (r=0.01) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    154\u001b[0m train_dataset_radius0_01, test_dataset_radius0_01, _, _ \u001b[38;5;241m=\u001b[39m load_and_preprocess_airfrans(graph_transform_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mradius\u001b[39m\u001b[38;5;124m'\u001b[39m, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, data_root\u001b[38;5;241m=\u001b[39mDATA_ROOT)\n\u001b[1;32m--> 155\u001b[0m analyze_graph_properties(train_dataset_radius0_01, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRadiusGraph (r=0.01) Training Set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# --- Experiment 1.4: RadiusGraph (r=0.05 - potentially more edges) ---\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Experiment 1.4: RadiusGraph (r=0.05) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[7], line 131\u001b[0m, in \u001b[0;36manalyze_graph_properties\u001b[1;34m(dataset, name)\u001b[0m\n\u001b[0;32m    129\u001b[0m num_nodes_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    130\u001b[0m num_edges_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m    132\u001b[0m     num_nodes_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_nodes\n\u001b[0;32m    133\u001b[0m     num_edges_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_edges\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:300\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[BaseData]:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m--> 300\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m[i]\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:292\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m    291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()[idx])\n\u001b[1;32m--> 292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_geometric\\transforms\\base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(copy\u001b[38;5;241m.\u001b[39mcopy(data))\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_geometric\\transforms\\compose.py:24\u001b[0m, in \u001b[0;36mCompose.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     22\u001b[0m         data \u001b[38;5;241m=\u001b[39m [transform(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m         data \u001b[38;5;241m=\u001b[39m transform(data)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_geometric\\transforms\\base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(copy\u001b[38;5;241m.\u001b[39mcopy(data))\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_geometric\\transforms\\radius_graph.py:43\u001b[0m, in \u001b[0;36mRadiusGraph.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Data:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m data\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     data\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;241m=\u001b[39m torch_geometric\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mradius_graph(\n\u001b[0;32m     44\u001b[0m         data\u001b[38;5;241m.\u001b[39mpos,\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr,\n\u001b[0;32m     46\u001b[0m         data\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop,\n\u001b[0;32m     48\u001b[0m         max_num_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_neighbors,\n\u001b[0;32m     49\u001b[0m         flow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow,\n\u001b[0;32m     50\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers,\n\u001b[0;32m     51\u001b[0m     )\n\u001b[0;32m     52\u001b[0m     data\u001b[38;5;241m.\u001b[39medge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\pool\\__init__.py:295\u001b[0m, in \u001b[0;36mradius_graph\u001b[1;34m(x, r, batch, loop, max_num_neighbors, flow, num_workers, batch_size)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mWITH_TORCH_CLUSTER_BATCH_SIZE:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_cluster\u001b[38;5;241m.\u001b[39mradius_graph(x, r, batch, loop, max_num_neighbors,\n\u001b[0;32m    294\u001b[0m                                       flow, num_workers)\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch_cluster\u001b[38;5;241m.\u001b[39mradius_graph(x, r, batch, loop, max_num_neighbors,\n\u001b[0;32m    296\u001b[0m                                   flow, num_workers, batch_size)\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_cluster\\radius.py:140\u001b[0m, in \u001b[0;36mradius_graph\u001b[1;34m(x, r, batch, loop, max_num_neighbors, flow, num_workers, batch_size)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes graph edges to all points within a given distance.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    edge_index = radius_graph(x, r=1.5, batch=batch, loop=False)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m flow \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_to_target\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_to_source\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 140\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m radius(x, x, r, batch, batch,\n\u001b[0;32m    141\u001b[0m                     max_num_neighbors,\n\u001b[0;32m    142\u001b[0m                     num_workers, batch_size, \u001b[38;5;129;01mnot\u001b[39;00m loop)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_to_target\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m], edge_index[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch_cluster\\radius.py:86\u001b[0m, in \u001b[0;36mradius\u001b[1;34m(x, y, r, batch_x, batch_y, max_num_neighbors, num_workers, batch_size, ignore_same_index)\u001b[0m\n\u001b[0;32m     83\u001b[0m     ptr_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbucketize(arange, batch_x)\n\u001b[0;32m     84\u001b[0m     ptr_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbucketize(arange, batch_y)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorch_cluster\u001b[38;5;241m.\u001b[39mradius(x, y, ptr_x, ptr_y, r,\n\u001b[0;32m     87\u001b[0m                                       max_num_neighbors, num_workers,\n\u001b[0;32m     88\u001b[0m                                       ignore_same_index)\n",
            "File \u001b[1;32mc:\\Users\\phani\\anaconda3\\Lib\\site-packages\\torch\\_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Graph Construction & Model Refinement - Step 1: Graph Construction Comparison\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import AirfRANS\n",
        "from torch_geometric.transforms import Compose, KNNGraph, RadiusGraph\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from torch_geometric.nn import GCNConv # Assuming GCNBaseline is still used for comparison\n",
        "\n",
        "# Define a root directory for the dataset (ensure this is defined in your notebook)\n",
        "# DATA_ROOT = './data/AirfRANS'\n",
        "\n",
        "# --- Custom Normalization Class (from previous) ---\n",
        "class CustomNormalize(object):\n",
        "    def __init__(self, x_scaler, y_scaler):\n",
        "        self.x_scaler = x_scaler\n",
        "        self.y_scaler = y_scaler\n",
        "\n",
        "    def __call__(self, data):\n",
        "        data.x = torch.tensor(self.x_scaler.transform(data.x.cpu().numpy()), dtype=torch.float)\n",
        "        data.y = torch.tensor(self.y_scaler.transform(data.y.cpu().numpy()), dtype=torch.float)\n",
        "        return data\n",
        "\n",
        "# --- Data Loading and Preprocessing Function (Modified fromprevious/Fix for MemoryError) ---\n",
        "# This function now also returns the scalers, which are needed for denormalization later\n",
        "def load_and_preprocess_airfrans(graph_transform_type='knn', k=10, r=0.1, data_root='./data/AirfRANS'):\n",
        "    print(f\"Loading AirfRANS dataset with {graph_transform_type} graph transform...\")\n",
        "\n",
        "    if graph_transform_type == 'knn':\n",
        "        graph_transform = KNNGraph(k=k)\n",
        "        print(f\"Using KNNGraph with k={k}\")\n",
        "    elif graph_transform_type == 'radius':\n",
        "        graph_transform = RadiusGraph(r=r)\n",
        "        print(f\"Using RadiusGraph with r={r}\")\n",
        "    else:\n",
        "        raise ValueError(\"graph_transform_type must be 'knn' or 'radius'\")\n",
        "\n",
        "    initial_train_dataset = AirfRANS(root=data_root, task='full', train=True, transform=None)\n",
        "\n",
        "    x_scaler = StandardScaler()\n",
        "    y_scaler = StandardScaler()\n",
        "\n",
        "    print(\"Fitting scalers incrementally using partial_fit()...\")\n",
        "    for i, data in enumerate(initial_train_dataset):\n",
        "        x_scaler.partial_fit(data.x.numpy())\n",
        "        y_scaler.partial_fit(data.y.numpy())\n",
        "        if (i + 1) % 100 == 0 or (i + 1) == len(initial_train_dataset):\n",
        "            print(f\"  Processed {i + 1}/{len(initial_train_dataset)} samples for scaler fitting.\")\n",
        "\n",
        "    normalize_transform = CustomNormalize(x_scaler, y_scaler)\n",
        "\n",
        "    train_transform_final = Compose([graph_transform, normalize_transform])\n",
        "    test_transform_final = Compose([graph_transform, normalize_transform])\n",
        "\n",
        "    train_dataset_preprocessed = AirfRANS(root=data_root, task='full', train=True, transform=train_transform_final)\n",
        "    test_dataset_preprocessed = AirfRANS(root=data_root, task='full', train=False, transform=test_transform_final)\n",
        "\n",
        "    print(f\"Loaded {len(train_dataset_preprocessed)} training samples after preprocessing.\")\n",
        "    print(f\"Loaded {len(test_dataset_preprocessed)} test samples after preprocessing.\")\n",
        "\n",
        "    return train_dataset_preprocessed, test_dataset_preprocessed, x_scaler, y_scaler\n",
        "\n",
        "# --- Baseline GNN Model (GCN, from previous - used for comparison) ---\n",
        "class GCNBaseline(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout_rate=0.5):\n",
        "        super(GCNBaseline, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            if i < self.num_layers - 1:\n",
        "                x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "# --- Training and Testing Functions (from previous) ---\n",
        "def train_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "def test_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# --- Main Execution  ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define DATA_ROOT (ensure this matches your environment)\n",
        "DATA_ROOT = './data/AirfRANS'\n",
        "\n",
        "# --- Experiment 1.1: KNNGraph (k=10, from Dprevious) ---\n",
        "print(\"\\n--- Experiment 1.1: KNNGraph (k=10) ---\")\n",
        "train_dataset_knn10, test_dataset_knn10, _, _ = load_and_preprocess_airfrans(graph_transform_type='knn', k=10, data_root=DATA_ROOT)\n",
        "\n",
        "# Analyze graph properties\n",
        "def analyze_graph_properties(dataset, name):\n",
        "    num_nodes_total = 0\n",
        "    num_edges_total = 0\n",
        "    for data in dataset:\n",
        "        num_nodes_total += data.num_nodes\n",
        "        num_edges_total += data.num_edges\n",
        "    avg_nodes_per_graph = num_nodes_total / len(dataset)\n",
        "    avg_edges_per_graph = num_edges_total / len(dataset)\n",
        "    avg_degree = (num_edges_total / num_nodes_total) if num_nodes_total > 0 else 0\n",
        "    print(f\"--- {name} Graph Properties ---\")\n",
        "    print(f\"Total graphs: {len(dataset)}\")\n",
        "    print(f\"Average nodes per graph: {avg_nodes_per_graph:.2f}\")\n",
        "    print(f\"Average edges per graph: {avg_edges_per_graph:.2f}\")\n",
        "    print(f\"Average degree: {avg_degree:.2f}\")\n",
        "\n",
        "analyze_graph_properties(train_dataset_knn10, \"KNNGraph (k=10) Training Set\")\n",
        "\n",
        "# --- Experiment 1.2: KNNGraph (k=20, more dense) ---\n",
        "print(\"\\n--- Experiment 1.2: KNNGraph (k=20) ---\")\n",
        "train_dataset_knn20, test_dataset_knn20, _, _ = load_and_preprocess_airfrans(graph_transform_type='knn', k=20, data_root=DATA_ROOT)\n",
        "analyze_graph_properties(train_dataset_knn20, \"KNNGraph (k=20) Training Set\")\n",
        "\n",
        "# --- Experiment 1.3: RadiusGraph (r=0.01 - try a smaller radius first) ---\n",
        "# Note: 'r' depends heavily on the scale of your 'pos' data. AirfRANS pos coordinates are often between 0 and 1.\n",
        "# A very small 'r' might result in disconnected graphs.\n",
        "print(\"\\n--- Experiment 1.3: RadiusGraph (r=0.01) ---\")\n",
        "train_dataset_radius0_01, test_dataset_radius0_01, _, _ = load_and_preprocess_airfrans(graph_transform_type='radius', r=0.01, data_root=DATA_ROOT)\n",
        "analyze_graph_properties(train_dataset_radius0_01, \"RadiusGraph (r=0.01) Training Set\")\n",
        "\n",
        "# --- Experiment 1.4: RadiusGraph (r=0.05 - potentially more edges) ---\n",
        "print(\"\\n--- Experiment 1.4: RadiusGraph (r=0.05) ---\")\n",
        "train_dataset_radius0_05, test_dataset_radius0_05, _, _ = load_and_preprocess_airfrans(graph_transform_type='radius', r=0.05, data_root=DATA_ROOT)\n",
        "analyze_graph_properties(train_dataset_radius0_05, \"RadiusGraph (r=0.05) Training Set\")\n",
        "\n",
        "\n",
        "# --- Run a quick training for each graph type for comparison ---\n",
        "print(\"\\n--- Running quick training for graph type comparison ---\")\n",
        "epochs_compare = 2 # Very short run for initial comparison\n",
        "hidden_channels_compare = 32\n",
        "dropout_rate_compare = 0.3\n",
        "learning_rate_compare = 0.001\n",
        "batch_size_compare = 1 # Keep batch size small for these large graphs\n",
        "\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "def run_comparison_train(train_ds, test_ds, name):\n",
        "    train_loader = DataLoader(train_ds[:5], batch_size=batch_size_compare, shuffle=True) # Subset for quickness\n",
        "    test_loader = DataLoader(test_ds[:2], batch_size=batch_size_compare, shuffle=False) # Subset for quickness\n",
        "\n",
        "    model = GCNBaseline(train_ds.num_node_features, hidden_channels_compare, train_ds[0].y.shape[1],\n",
        "                        num_layers=3, dropout_rate=dropout_rate_compare).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_compare)\n",
        "\n",
        "    print(f\"\\n--- Training with {name} ---\")\n",
        "    for epoch in range(1, epochs_compare + 1):\n",
        "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
        "        test_loss = test_model(model, test_loader, criterion, device)\n",
        "        print(f'  Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "run_comparison_train(train_dataset_knn10, test_dataset_knn10, \"KNNGraph (k=10)\")\n",
        "run_comparison_train(train_dataset_knn20, test_dataset_knn20, \"KNNGraph (k=20)\")\n",
        "run_comparison_train(train_dataset_radius0_01, test_dataset_radius0_01, \"RadiusGraph (r=0.01)\")\n",
        "run_comparison_train(train_dataset_radius0_05, test_dataset_radius0_05, \"RadiusGraph (r=0.05)\")\n",
        "\n",
        "print(\"\\n Graph Construction Comparison complete. Analyze the graph properties and initial training losses to inform your choice for further refinement.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkk"
      ],
      "metadata": {
        "id": "6AxB_dbsnjFS"
      },
      "id": "6AxB_dbsnjFS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}